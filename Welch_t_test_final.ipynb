{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The below line was causing Java gateway Process exited before sending its port number error\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars $PKG_HOME/snowflake-jdbc-3.6.10.jar,$PKG_HOME/spark-snowflake_2.11-2.4.4-spark_2.2.jar,$PKG_HOME/hadoop-aws-2.7.3.jar,$PKG_HOME/aws-java-sdk-1.7.4.jar'\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.types as sql_types\n",
    "import pyspark.sql.functions as sql_functions\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "spark_conf = SparkConf().setMaster('local').setAppName('Stat Sig T-Test S3 Parquet')\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Some constants\n",
    "#\n",
    "aws_profile = \"prasad_dev\"\n",
    "aws_region = \"us-east-1\"\n",
    "s3_bucket = \"ou-dev-workspace\"\n",
    "\n",
    "# \n",
    "# Reading environment variables from aws credential file \n",
    "#\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\") \n",
    "\n",
    "\n",
    "# You might need to set these\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", access_id)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", access_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = s3_bucket + \"/prasad/welch_t_test/input_data/*\"\n",
    "df3=spark.read.parquet(\"s3a://\" + path1)\n",
    "df3.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_t_test(mean_c, var_c, n_c, mean_t, var_t, n_t):\n",
    "    '''\n",
    "        returns t-statistic, two_tail p-value and degrees_of_freedom\n",
    "    '''\n",
    "\n",
    "    # t = float((mean_c - mean_t) / ((((float(n_c - 1) * var_c + float(n_t - 1) * var_t) / float(n_c + n_t - 2)) ** (1/2)) * float(1 / n_c + 1 / n_t) ** (1/2)))\n",
    "    t = (mean_c - mean_t) / ((var_c/float(n_c) + var_t/float(n_t)) ** (1/2))\n",
    "\n",
    "    #Degrees of freedom\n",
    "    df = float(n_c + n_t - 2)\n",
    "\n",
    "    #p-value after comparison with the t \n",
    "    p = float(stats.t.cdf(t,df=df))\n",
    "    \n",
    "    return sql_types.Row('T_STATISTIC', 'TWO_TAIL_P_VALUE', 'DEGREES_OF_FREEDOM')(round(t, 6), p, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "welch_t_test_schema = sql_types.StructType([\n",
    "    sql_types.StructField(\"T_STATISTIC\", sql_types.DoubleType(), True),\n",
    "    sql_types.StructField(\"TWO_TAIL_P_VALUE\", sql_types.DoubleType(), True),\n",
    "    sql_types.StructField(\"DEGREES_OF_FREEDOM\", sql_types.DoubleType(), True)])\n",
    "\n",
    "welch_t_test_udf = sql_functions.udf(welch_t_test, welch_t_test_schema)\n",
    "\n",
    "welch_1k_t_test_schema = sql_types.StructType([\n",
    "    sql_types.StructField(\"T_STATISTIC_1K\", sql_types.DoubleType(), True),\n",
    "    sql_types.StructField(\"TWO_TAIL_P_VALUE_1K\", sql_types.DoubleType(), True),\n",
    "    sql_types.StructField(\"DEGREES_OF_FREEDOM_1K\", sql_types.DoubleType(), True)])\n",
    "\n",
    "welch_1k_t_test_udf = sql_functions.udf(welch_t_test, welch_1k_t_test_schema)\n",
    "\n",
    "welch_values_df = df3.withColumn(\"WELCH_T_VALUES\", \n",
    "            sql_functions.explode(\n",
    "                sql_functions.array(\n",
    "                    welch_t_test_udf(\n",
    "                         df3[\"MEAN_METRIC_VALUE_IN_CONTROL\"],\n",
    "                         df3[\"VARIANCE_OF_METRIC_VALUE_IN_CONTROL\"],\n",
    "                         df3[\"USERS_BUCKETED_IN_CONTROL\"],\n",
    "                         df3[\"MEAN_METRIC_VALUE_IN_TREATMENT\"],\n",
    "                         df3[\"VARIANCE_OF_METRIC_VALUE_IN_TREATMENT\"],\n",
    "                         df3[\"USERS_BUCKETED_IN_TREATMENT\"]\n",
    "                    )))).withColumn(\"WELCH_T_VALUES_1K\", \n",
    "            sql_functions.explode(\n",
    "                sql_functions.array(\n",
    "                    welch_1k_t_test_udf(\n",
    "                         df3[\"MEAN_METRIC_VALUE_IN_CONTROL\"]/1000,\n",
    "                         df3[\"VARIANCE_OF_METRIC_VALUE_IN_CONTROL\"],\n",
    "                         df3[\"USERS_BUCKETED_IN_CONTROL\"],\n",
    "                         df3[\"MEAN_METRIC_VALUE_IN_TREATMENT\"]/1000,\n",
    "                         df3[\"VARIANCE_OF_METRIC_VALUE_IN_TREATMENT\"],\n",
    "                         df3[\"USERS_BUCKETED_IN_TREATMENT\"]\n",
    "                    )))).withColumn(\"MEAN_METRIC_VALUE_IN_CONTROL_1K\",\n",
    "                        df3[\"MEAN_METRIC_VALUE_IN_CONTROL\"]/1000).withColumn(\"MEAN_METRIC_VALUE_IN_TREATMENT_1K\",\n",
    "                                                                             df3[\"MEAN_METRIC_VALUE_IN_TREATMENT\"]/1000)\n",
    "\n",
    "\n",
    "all_columns_df_1k = welch_values_df.select(\n",
    "                \"CAL_DATE\",\n",
    "                 \"EXPERIMENT_ID\",\n",
    "                 \"EXPERIMENT_NAME\",\n",
    "                 \"CONTROL_VARIANT_ID\",\n",
    "                 \"CONTROL_VARIANT_NAME\",\n",
    "                 \"TREATMENT_VARIANT_ID\",\n",
    "                 \"TREATMENT_VARIANT_NAME\",\n",
    "                 \"METRIC_NAME\",\n",
    "                 \"USERS_BUCKETED_IN_CONTROL\",\n",
    "                 \"VARIANCE_OF_METRIC_VALUE_IN_CONTROL\",\n",
    "                 \"MEAN_METRIC_VALUE_IN_CONTROL\",\n",
    "                 \"USERS_BUCKETED_IN_TREATMENT\",\n",
    "                 \"VARIANCE_OF_METRIC_VALUE_IN_TREATMENT\",\n",
    "                 \"MEAN_METRIC_VALUE_IN_TREATMENT\",\n",
    "                 \"WELCH_T_VALUES.*\",\n",
    "                 \"WELCH_T_VALUES_1K.T_STATISTIC_1K\",\n",
    "                 \"WELCH_T_VALUES_1K.TWO_TAIL_P_VALUE_1K\",\n",
    "                 \"MEAN_METRIC_VALUE_IN_CONTROL_1K\",\n",
    "                 \"MEAN_METRIC_VALUE_IN_TREATMENT_1K\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, EXPERIMENT_ID: string, EXPERIMENT_NAME: string, CONTROL_VARIANT_ID: string, CONTROL_VARIANT_NAME: string, TREATMENT_VARIANT_ID: string, TREATMENT_VARIANT_NAME: string, METRIC_NAME: string, USERS_BUCKETED_IN_CONTROL: string, VARIANCE_OF_METRIC_VALUE_IN_CONTROL: string, MEAN_METRIC_VALUE_IN_CONTROL: string, USERS_BUCKETED_IN_TREATMENT: string, VARIANCE_OF_METRIC_VALUE_IN_TREATMENT: string, MEAN_METRIC_VALUE_IN_TREATMENT: string, T_STATISTIC: string, TWO_TAIL_P_VALUE: string, DEGREES_OF_FREEDOM: string, T_STATISTIC_1K: string, TWO_TAIL_P_VALUE_1K: string, MEAN_METRIC_VALUE_IN_CONTROL_1K: string, MEAN_METRIC_VALUE_IN_TREATMENT_1K: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns_df_1k.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = s3_bucket + \"/prasad/welch_t_test/output_data_1k/\"\n",
    "all_columns_df_1k.write.parquet(\"s3a://\" + output_path,mode=\"overwrite\")\n",
    "\n",
    "output_path2 = s3_bucket + \"/prasad/welch_t_test/output_data_1k_csv/\"\n",
    "all_columns_df_1k.write.csv(\"s3a://\" + output_path2,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
